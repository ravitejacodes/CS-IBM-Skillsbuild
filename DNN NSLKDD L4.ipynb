{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "1.26.4\n",
      "3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]\n",
      "1.4.2\n",
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(sys.version)\n",
    "print(sklearn.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\91630\\AppData\\Local\\Temp\\ipykernel_10324\\1092257020.py:15: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  df_test = pd.read_csv(\"C:\\\\Users\\\\91630\\Desktop\\\\subjects pdf\\\\KDDTest+.csv\", header=None, names = col_names)\n",
      "C:\\Users\\91630\\AppData\\Local\\Temp\\ipykernel_10324\\1092257020.py:15: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  df_test = pd.read_csv(\"C:\\\\Users\\\\91630\\Desktop\\\\subjects pdf\\\\KDDTest+.csv\", header=None, names = col_names)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m col_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflag\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst_bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mland\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrong_fragment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murgent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhot\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_failed_logins\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogged_in\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_compromised\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot_shell\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msu_attempted\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_root\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst_host_srv_diff_host_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst_host_serror_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst_host_srv_serror_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst_host_rerror_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst_host_srv_rerror_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# these have already been removed.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m91630\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msubjects pdf\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKDDTrain+.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names \u001b[38;5;241m=\u001b[39m col_names)\n\u001b[0;32m     15\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m91630\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msubjects pdf\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mKDDTest+.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names \u001b[38;5;241m=\u001b[39m col_names)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# shape, this gives the dimensions of the dataset\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
    "\n",
    "# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\n",
    "# these have already been removed.\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\91630\\\\Desktop\\\\subjects pdf\\\\KDDTrain+.csv\", header=None, names = col_names)\n",
    "df_test = pd.read_csv(\"C:\\\\Users\\\\91630\\Desktop\\\\subjects pdf\\\\KDDTest+.csv\", header=None, names = col_names)\n",
    "\n",
    "# shape, this gives the dimensions of the dataset\n",
    "print('Dimensions of the Training set:',df.shape)\n",
    "print('Dimensions of the Test set:',df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 70 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 23 categories\n",
      "\n",
      "Distribution of categories in protocol type:\n",
      "protocol_type\n",
      "tcp     102689\n",
      "udp      14993\n",
      "icmp      8291\n",
      "Name: count, dtype: int64\n",
      "Distribution of categories in service type:\n",
      "service\n",
      "http        40338\n",
      "private     21853\n",
      "domain_u     9043\n",
      "smtp         7313\n",
      "ftp_data     6860\n",
      "Name: count, dtype: int64\n",
      "Distribution of categories in flag type:\n",
      "flag\n",
      "SF      74945\n",
      "S0      34851\n",
      "REJ     11233\n",
      "RSTR     2421\n",
      "RSTO     1562\n",
      "Name: count, dtype: int64\n",
      "Distribution of categories in label type:\n",
      "label\n",
      "normal       67343\n",
      "neptune      41214\n",
      "satan         3633\n",
      "ipsweep       3599\n",
      "portsweep     2931\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Identify categorical features\n",
    "# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n",
    "# explore categorical features\n",
    "print('Training set:')\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\n",
    "print()\n",
    "print('Distribution of categories in protocol type:')\n",
    "print(df['protocol_type'].value_counts().sort_values(ascending=False).head())\n",
    "print('Distribution of categories in service type:')\n",
    "print(df['service'].value_counts().sort_values(ascending=False).head())\n",
    "print('Distribution of categories in flag type:')\n",
    "print(df['flag'].value_counts().sort_values(ascending=False).head())\n",
    "print('Distribution of categories in label type:')\n",
    "print(df['label'].value_counts().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 63 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 38 categories\n"
     ]
    }
   ],
   "source": [
    "print('Test set:')\n",
    "for col_name in df_test.columns:\n",
    "    if df_test[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df_test[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  protocol_type   service flag\n",
       "0           tcp  ftp_data   SF\n",
       "1           udp     other   SF\n",
       "2           tcp   private   S0\n",
       "3           tcp      http   SF\n",
       "4           tcp      http   SF"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "# insert code to get a list of categorical columns into a variable, categorical_columns\n",
    "categorical_columns=['protocol_type', 'service', 'flag'] \n",
    " # Get the categorical values into a 2D numpy array\n",
    "df_categorical_values = df[categorical_columns]\n",
    "testdf_categorical_values = df_test[categorical_columns]\n",
    "df_categorical_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   protocol_type  service  flag\n",
      "0              1       20     9\n",
      "1              2       44     9\n",
      "2              1       49     5\n",
      "3              1       24     9\n",
      "4              1       24     9\n"
     ]
    }
   ],
   "source": [
    "df_categorical_values_enc=df_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "print(df_categorical_values_enc.head())\n",
    "# test set\n",
    "testdf_categorical_values_enc=testdf_categorical_values.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   protocol_type  service  flag\n",
       "0              1       20     9\n",
       "1              2       44     9\n",
       "2              1       49     5\n",
       "3              1       24     9\n",
       "4              1       24     9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categorical_values_enc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
      "0         0        491          0     0               0       0    0   \n",
      "1         0        146          0     0               0       0    0   \n",
      "2         0          0          0     0               0       0    0   \n",
      "3         0        232       8153     0               0       0    0   \n",
      "4         0        199        420     0               0       0    0   \n",
      "\n",
      "   num_failed_logins  logged_in  num_compromised  ...  \\\n",
      "0                  0          0                0  ...   \n",
      "1                  0          0                0  ...   \n",
      "2                  0          0                0  ...   \n",
      "3                  0          1                0  ...   \n",
      "4                  0          1                0  ...   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.17                         0.00   \n",
      "1                         0.88                         0.00   \n",
      "2                         0.00                         0.00   \n",
      "3                         0.03                         0.04   \n",
      "4                         0.00                         0.00   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                  0.05   \n",
      "1                  0.00                      0.00                  0.00   \n",
      "2                  1.00                      1.00                  0.00   \n",
      "3                  0.03                      0.01                  0.00   \n",
      "4                  0.00                      0.00                  0.00   \n",
      "\n",
      "   dst_host_srv_rerror_rate    label  protocol_type  service  flag  \n",
      "0                      0.00   normal              1       20     9  \n",
      "1                      0.00   normal              2       44     9  \n",
      "2                      0.00  neptune              1       49     5  \n",
      "3                      0.01   normal              1       24     9  \n",
      "4                      0.00   normal              1       24     9  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "(125973, 42)\n",
      "(22543, 42)\n"
     ]
    }
   ],
   "source": [
    "df.drop('flag', axis=1, inplace=True)\n",
    "df.drop('protocol_type', axis=1, inplace=True)\n",
    "df.drop('service', axis=1, inplace=True)\n",
    "df=df.join(df_categorical_values_enc)\n",
    "#print(newdf)\n",
    "# test data\n",
    "df_test.drop('flag', axis=1, inplace=True)\n",
    "df_test.drop('protocol_type', axis=1, inplace=True)\n",
    "df_test.drop('service', axis=1, inplace=True)\n",
    "df_test=df_test.join(testdf_categorical_values_enc)\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "#print(newdf_test)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4\n",
      "1    4\n",
      "2    0\n",
      "3    4\n",
      "4    4\n",
      "Name: label, dtype: int64\n",
      "0    4\n",
      "1    4\n",
      "2    0\n",
      "3    4\n",
      "4    4\n",
      "Name: label, dtype: int64\n",
      "   duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
      "0         0        491          0     0               0       0    0   \n",
      "1         0        146          0     0               0       0    0   \n",
      "2         0          0          0     0               0       0    0   \n",
      "3         0        232       8153     0               0       0    0   \n",
      "4         0        199        420     0               0       0    0   \n",
      "\n",
      "   num_failed_logins  logged_in  num_compromised  ...  \\\n",
      "0                  0          0                0  ...   \n",
      "1                  0          0                0  ...   \n",
      "2                  0          0                0  ...   \n",
      "3                  0          1                0  ...   \n",
      "4                  0          1                0  ...   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.17                         0.00   \n",
      "1                         0.88                         0.00   \n",
      "2                         0.00                         0.00   \n",
      "3                         0.03                         0.04   \n",
      "4                         0.00                         0.00   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                  0.05   \n",
      "1                  0.00                      0.00                  0.00   \n",
      "2                  1.00                      1.00                  0.00   \n",
      "3                  0.03                      0.01                  0.00   \n",
      "4                  0.00                      0.00                  0.00   \n",
      "\n",
      "   dst_host_srv_rerror_rate  label  protocol_type  service  flag  \n",
      "0                      0.00      4              1       20     9  \n",
      "1                      0.00      4              2       44     9  \n",
      "2                      0.00      0              1       49     5  \n",
      "3                      0.01      4              1       24     9  \n",
      "4                      0.00      4              1       24     9  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "(22543, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91630\\AppData\\Local\\Temp\\ipykernel_21308\\549405909.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  newlabeldf=labeldf.replace({ 'normal' : 4, 'neptune' : 0 ,'back': 0, 'land': 0, 'pod': 0, 'smurf': 0, 'teardrop': 0,'mailbomb': 0, 'apache2': 0, 'processtable': 0, 'udpstorm': 0, 'worm': 0,\n",
      "C:\\Users\\91630\\AppData\\Local\\Temp\\ipykernel_21308\\549405909.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  newlabeldf_test=labeldf_test.replace({ 'normal' : 4, 'neptune' : 0 ,'back': 0, 'land': 0, 'pod': 0, 'smurf': 0, 'teardrop': 0,'mailbomb': 0, 'apache2': 0, 'processtable': 0, 'udpstorm': 0, 'worm': 0,\n"
     ]
    }
   ],
   "source": [
    "# take label column\n",
    "labeldf=df['label']\n",
    "labeldf_test=df_test['label']\n",
    "\n",
    "# change the label column\n",
    "newlabeldf=labeldf.replace({ 'normal' : 4, 'neptune' : 0 ,'back': 0, 'land': 0, 'pod': 0, 'smurf': 0, 'teardrop': 0,'mailbomb': 0, 'apache2': 0, 'processtable': 0, 'udpstorm': 0, 'worm': 0,\n",
    "                           'ipsweep' : 1,'nmap' : 1,'portsweep' : 1,'satan' : 1,'mscan' : 1,'saint' : 1\n",
    "                           ,'ftp_write': 2,'guess_passwd': 2,'imap': 2,'multihop': 2,'phf': 2,'spy': 2,'warezclient': 2,'warezmaster': 2,'sendmail': 2,'named': 2,'snmpgetattack': 2,'snmpguess': 2,'xlock': 2,'xsnoop': 2,'httptunnel': 2,\n",
    "                           'buffer_overflow': 3,'loadmodule': 3,'perl': 3,'rootkit': 3,'ps': 3,'sqlattack': 3,'xterm': 3})\n",
    "newlabeldf_test=labeldf_test.replace({ 'normal' : 4, 'neptune' : 0 ,'back': 0, 'land': 0, 'pod': 0, 'smurf': 0, 'teardrop': 0,'mailbomb': 0, 'apache2': 0, 'processtable': 0, 'udpstorm': 0, 'worm': 0,\n",
    "                           'ipsweep' : 1,'nmap' : 1,'portsweep' : 1,'satan' : 1,'mscan' : 1,'saint' : 1\n",
    "                           ,'ftp_write': 2,'guess_passwd': 2,'imap': 2,'multihop': 2,'phf': 2,'spy': 2,'warezclient': 2,'warezmaster': 2,'sendmail': 2,'named': 2,'snmpgetattack': 2,'snmpguess': 2,'xlock': 2,'xsnoop': 2,'httptunnel': 2,\n",
    "                           'buffer_overflow': 3,'loadmodule': 3,'perl': 3,'rootkit': 3,'ps': 3,'sqlattack': 3,'xterm': 3})\n",
    "# put the new label column back\n",
    "print(newlabeldf.head())\n",
    "df['label'] = newlabeldf\n",
    "df_test['label'] = newlabeldf_test\n",
    "print(df['label'].head())\n",
    "print(df.head())\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 41)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df\n",
    "data = data.drop(\"label\", axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22543, 41)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test=df_test\n",
    "data_test= data_test.drop(\"label\", axis=1)\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =MinMaxScaler()\n",
    "scaled_data =scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =MinMaxScaler()\n",
    "scaled_data_test =scaler.fit_transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=scaled_data\n",
    "df_test=scaled_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 41)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22543, 41)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data=np.column_stack((df,newlabeldf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data=np.column_stack((df,newlabeldf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_test=np.column_stack((df_test,newlabeldf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 3.55806412e-07, 0.00000000e+00, ...,\n",
       "        2.89855072e-01, 9.00000000e-01, 4.00000000e+00],\n",
       "       [0.00000000e+00, 1.05799870e-07, 0.00000000e+00, ...,\n",
       "        6.37681159e-01, 9.00000000e-01, 4.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        7.10144928e-01, 5.00000000e-01, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 1.61670897e-06, 2.93143779e-07, ...,\n",
       "        7.82608696e-01, 9.00000000e-01, 4.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        4.34782609e-01, 5.00000000e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.09423153e-07, 0.00000000e+00, ...,\n",
       "        2.89855072e-01, 9.00000000e-01, 4.00000000e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22543, 42)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91630\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.01}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m dnn\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m))          \n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#model.add(Activation('softmax'))\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# define optimizer and objective, compile cnn\u001b[39;00m\n\u001b[0;32m     81\u001b[0m dnn\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mopt,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\91630\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:60\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, name, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     59\u001b[0m ):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n",
      "File \u001b[1;32mc:\\Users\\91630\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:22\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[1;32mc:\\Users\\91630\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:38\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[1;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m     )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 0.01}"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras  import utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "traindata = df_data\n",
    "testdata = df_data_test\n",
    "\n",
    "\n",
    "X = traindata[:,1:42]\n",
    "Y = traindata[:,41]\n",
    "C = testdata[:,41]\n",
    "T = testdata[:,1:42]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "   \n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "\n",
    "#y_train= to_categorical(y_train1)\n",
    "#y_test= to_categorical(y_test1)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1]))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1]))\n",
    "\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "dnn = Sequential()\n",
    "\n",
    "dnn.add(Dense(64, activation='relu',input_shape=(41,)))\n",
    "dnn.add(Dense(64, activation='relu'))\n",
    "dnn.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(5)\n",
    "dnn.add(Dense(5, activation=\"softmax\"))          \n",
    "#model.add(Activation('softmax'))\n",
    "opt = Adam(lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "dnn.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"D:/Results/II/NSLKDD/DNN-L3/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('D:/Results/II/NSLKDD/DNN-L3/dnnanalysis.csv',separator=',', append=False)\n",
    "#model.fit(X_train, y_train, epochs=10, verbose=1)\n",
    "#model.fit(X_train, y_train, batch_size=batch_size, epochs=500, validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "dnn.fit(X_train, y_train, nb_epoch=50, validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "\n",
    "  \n",
    "dnn.save(\"D:/Results/II/NSLKDD/DNN-L3/dnn_model.hdf5\")\n",
    "\n",
    "loss, accuracy = dnn.evaluate(X_test,y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "y_pred = dnn.predict_classes(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 14.7 GiB for an array with shape (393665625, 5) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 58\u001b[0m\n\u001b[0;32m     52\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(testT, (testT\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],testT\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     56\u001b[0m y_test\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(C)\n\u001b[1;32m---> 58\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m y_test \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(y_test, num_classes)\n\u001b[0;32m     62\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\91630\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\numerical_utils.py:88\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(x, num_classes)\u001b[0m\n\u001b[0;32m     86\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     87\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 88\u001b[0m categorical \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(batch_size), x] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     90\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 14.7 GiB for an array with shape (393665625, 5) and data type float64"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras import utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "\n",
    "#traindata = pd.read_csv('kdd/multiclass/kddtrain.csv', header=None)\n",
    "testdata = df_data_test\n",
    "\n",
    "\n",
    "#X = traindata.iloc[:,0:42]\n",
    "#Y = traindata.iloc[:,0]\n",
    "C = testdata[:,41]\n",
    "T = testdata[:,1:42]\n",
    "\n",
    "#scaler = Normalizer().fit(X)\n",
    "#trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "#y_train1 = np.array(Y)\n",
    "y_test= np.array(C)\n",
    "\n",
    "\n",
    "#y_train= to_categorical(y_train1)\n",
    "#y_test= to_categorical(y_test1)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "#X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "y_test= np.array(C)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "dnn = Sequential()\n",
    "\n",
    "dnn.add(Dense(64, activation='relu',input_shape=(41,)))\n",
    "dnn.add(Dense(64, activation='relu'))\n",
    "dnn.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(5)\n",
    "dnn.add(Dense(5, activation=\"softmax\"))          \n",
    "#model.add(Activation('softmax'))\n",
    "#opt = Adam(lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "#model.add(Activation('softmax'))\n",
    "#opt=Adam(lr=0.01)\n",
    "\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "'''\n",
    "cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn3results/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn3results/cnntrainanalysis3.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=1000, show_accuracy=True,validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn3results/cnn_model.hdf5\")\n",
    "'''\n",
    "\n",
    "dnn.load_weights(\"C:\\\\Users\\\\91630\\\\Desktop\\\\subjects pdf\\\\dnn_model.hdf5\")\n",
    "\n",
    "y_pred = dnn.predict_classes(X_test)\n",
    "\n",
    "np.savetxt(\"C:\\\\Users\\\\91630\\\\Desktop\\\\subjects pdf\\\\ExceptResult.txt\", y_test, fmt='%01d')\n",
    "np.savetxt(\"C:\\\\Users\\\\91630\\\\Desktop\\\\subjects pdf\\\\PrediticonResult.txt\", y_pred, fmt='%01d')\n",
    "\n",
    "dnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "loss, accuracy = dnn.evaluate(X_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = dnn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lw = 2\n",
    "num_classes =5\n",
    "\n",
    "fpr =dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i],tpr[i],_ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr[\"micro\"],tpr[\"micro\"],_ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= num_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label = 'micro-average ROC curve (area ={0:0.2f})'.format(roc_auc[\"micro\"]),\n",
    "         color = 'deeppink', linestyle=':', linewidth = 4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red','blue'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label='ROC curve of class DoS (area = {1:0.2f})'\n",
    "        ' '.format(i, roc_auc[i]))\n",
    "    if i==1:\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label='ROC curve of class Prob (area = {1:0.2f})'\n",
    "        ''.format(i, roc_auc[i]))\n",
    "    if i==2:\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label='ROC curve of class R2L (area = {1:0.2f})'\n",
    "        ''.format(i, roc_auc[i]))\n",
    "    if i==3:\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label='ROC curve of class U2R (area = {1:0.2f})'\n",
    "        ''.format(i, roc_auc[i]))\n",
    "    if i==4:\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label='ROC curve of class Normal (area = {1:0.2f})'\n",
    "        ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = np.genfromtxt ('D:/Results/II/NSLKDD/DNN-L3/res/predicted.txt', delimiter=\" \")\n",
    "expected = np.genfromtxt ('D:/Results/II/NSLKDD/DNN-L3/res/expected.txt', delimiter=\" \")\n",
    "y_true = expected[:]\n",
    "y_pred = predicted[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_true,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network-Intrusion-Detection/NSL-KDD/LSTM/multiclass/confmat.py\n",
    "#import pandas as pd\n",
    "#import pandas_ml as pdml\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlxtend                                                          \n",
    "print(mlxtend.__version__)  \n",
    "\n",
    "#import pystan\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "#class_names['DoS', 'Prob', 'R2L', 'U2R', 'Normal']\n",
    "y_true = np.argmax(y_true,axis=1)\n",
    "#print(np.count_nonzero(y_true <> 2))\n",
    "cm = confusion_matrix(y_true,y_pred)\n",
    "print(cm)\n",
    "\n",
    "cm = pd.DataFrame(cm,\n",
    "                     index = ['DoS','Prob','R2L','U2R','Normal'], \n",
    "                     columns = ['DoS','Prob','R2L','U2R','Normal'])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "#plt.title('CNN  \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#tn, fp, fn, tp = confusion_matrix(new_y_true, new_y_pred).ravel()\n",
    "\n",
    "#print(\" TP:\\t\" + str(tp) + \"\\nTN:\\t\" + str(tn) + \"\\nFP:\\t\" + str(fp) + \"\\nFN:\\t\" + str(fn))\n",
    "\n",
    "y_true1 = np.array(y_true)\n",
    "y_pred1 = np.array(y_pred)\n",
    "\n",
    "TP=0\n",
    "FP=0\n",
    "TN=0\n",
    "FN=0\n",
    "\n",
    "for i in range(len(y_pred1)): \n",
    "        if y_true1[i]==y_pred1[i] and y_pred1[i]!=4:\n",
    "           TP += 1\n",
    "        if y_pred1[i]!=4 and y_true1[i]!=y_pred1[i]:\n",
    "           FP += 1\n",
    "        if y_true1[i]==y_pred1[i] and y_pred1[i]==4:\n",
    "           TN += 1\n",
    "        if y_true1[i]<4 and y_pred1[i]==4:\n",
    "           FN += 1\n",
    "\n",
    "print(\" TP:\\t\" + str(TP) + \"\\nTN:\\t\" + str(TN) + \"\\nFP:\\t\" + str(FP) + \"\\nFN:\\t\" + str(FN))\n",
    "\n",
    "print(accuracy_score(y_true1, y_pred1))\n",
    "#print(cm)\n",
    "\n",
    "TPR = TP/(TP+FN)\n",
    "    # Precision = TP/(TP+FP)\n",
    "PREC = TP/(TP+FP)\n",
    "    # False Positive Rate (FPR) = FP/(FP+TN)\n",
    "FPR = FP/(FP+TN)\n",
    "    # Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "ACC = (TP+TN)/(TP+FP+TN+FN)\n",
    "        # recall \n",
    "REC = TP/(TP+FN)\n",
    "\n",
    "FAR = FP/(FP+TN)\n",
    "\n",
    "        \n",
    "    #F1=2*((Precision*Recall)/(Precision+Recall))\n",
    "F1=2*((PREC*REC)/(PREC+REC))\n",
    "print(\"Accuracy = \\t\" + str(ACC) +\"\\nFPR= \\t \"+ str(FPR) + \"\\nTPR= \\t\" + str(TPR)+ \"\\nPrecision= \\t\"+str(PREC)+\"\\nRecall= \\t\"+str(REC)+\"\\n F1= \\t\"+str(F1)+\"\\n FAR= \\t\"+str(FAR))\n",
    "   \n",
    "print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "target_names = ['class 0', 'class 1', 'class 2','class3','class4']\n",
    "#print(\"**************Built-In****************************\")\n",
    "#print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "#print(\"*************************************************\").tp = tp.astype(float)\n",
    "cl = 0;\n",
    "for num in range(0,5):\n",
    "    new_y_true = y_true.astype(int)\n",
    "    new_y_pred = y_pred.astype(int)\n",
    "\n",
    "    #Convert one class as 1 and rest of the classes as 0\n",
    "    TP=0\n",
    "    FP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    Total=0\n",
    "    print(\"Class label\",cl)\n",
    "    for i in range(len(y_pred1)): \n",
    "        if y_true1[i]==y_pred1[i]==cl:\n",
    "           TP += 1\n",
    "        if y_pred1[i]!=cl and y_true1[i]!=y_pred1[i]:\n",
    "           FP += 1\n",
    "        if y_true1[i]!=cl and y_pred1[i]==cl:\n",
    "           TN += 1\n",
    "           FN += 1\n",
    "        \n",
    "    #print(TP, FP, TN, FN)\n",
    "    Total=(TP+TN)\n",
    "    \n",
    "    print(\" TP:\\t\" + str(TP) + \"\\nTN:\\t\" + str(TN) + \"\\nFP:\\t\" + str(FP) + \"\\nFN:\\t\" + str(FN)+\"\\nTotal Labels\\t\"+ str(Total))\n",
    "    Accuracy=TP/(TP+TN)\n",
    "    print(\"Accuracy=\",Accuracy)\n",
    "    cl=cl+1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
